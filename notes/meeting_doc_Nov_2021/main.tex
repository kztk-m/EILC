\documentclass{article}

\usepackage{fullpage}

\usepackage{textcomp}

\usepackage{xspace}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{proof}
\usepackage{polytable}
\usepackage{alltt}
\usepackage{calc}

\usepackage[round,sort]{natbib}
\let\cite=\citep

\input{macro}

\usepackage{amsthm} 
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\NewDocumentCommand{\figbox}{m}{%
\setlength{\fboxsep}{2pt}%
\fbox{\parbox{\textwidth-2\fboxsep-2pt}{%
\setlength{\mathindent}{0pt}%
\setlength{\abovedisplayskip}{0pt}%
\setlength{\belowdisplayskip}{0pt}%
#1}}}

% Taken from https://tex.stackexchange.com/a/133093
\makeatletter
\newcommand*{\pmzerodot}{%
  \nfss@text{%
    \sbox0{$\vcenter{}$}% math axis
    \sbox2{0}%
    \sbox4{0\/}%
    \ooalign{%
      0\cr
      \hidewidth
      \kern\dimexpr\wd4-\wd2\relax % compensate for slanted fonts
      \raise\dimexpr(\ht2-\dp2)/2-\ht0\relax\hbox{%
        \if b\expandafter\@car\f@series\@nil\relax
          \mathversion{bold}%
        \fi
        $\cdot\m@th$%
      }%
      \hidewidth
      \cr
      \vphantom{0}% correct depth of final symbol
    }%
  }%
}
\newcommand*{\pmzeroslash}{%
  \nfss@text{%
    \sbox0{0}%
    \sbox2{/}%
    \sbox4{%
      \raise\dimexpr((\ht0-\dp0)-(\ht2-\dp2))/2\relax\copy2 %
    }%
    \ooalign{%
      \hfill\copy4 \hfill\cr
      \hfill0\hfill\cr
    }%
    \vphantom{0\copy4 }% correct overall height and depth of the symbol
  }%
}
\makeatother

\makeatletter
%mathindent has to be defined
\@ifundefined{mathindent}%
  {\newdimen\mathindent\mathindent\leftmargini}%
  {}%
\makeatother
\setlength\mathindent{1.8em plus 0em minus 0.2em}%{1.5em plus 0em minus 0.5em}
\newlength{\blanklineskip}
\setlength{\blanklineskip}{0.66084ex}

\usepackage{listings}
\lstloadlanguages{Haskell}
\lstnewenvironment{hscode}%
{\lstset{
   language=Haskell, 
%   keywordstyle=\bfseries\rmfamily\upshape, 
   basicstyle=\small\ttfamily, 
   flexiblecolumns=false,
   literate={+}{{$+$}}1 {/}{{$/$}}1 {*}{{$*$}}1 {=}{{$=$}}1
            {>}{{$>$}}1 {<}{{$<$}}1 {\\}{{$\lambda$}}1
            {\\\\}{{\char`\\\char`\\}}1
            {->}{{$\rightarrow$}}2 {>=}{{$\geq$}}2 {<-}{{$\leftarrow$}}2
            {<=}{{$\leq$}}2 {=>}{{$\Rightarrow$}}2 
            {\ .}{{$\circ$}}2 {\ .\ }{{$\circ$}}2
            {>>}{{>>}}2 {>>=}{{>>=}}2 
%            {::}{{${::}$}}1 {:}{{$:$}}1
            {|}{{$\mid$}}1               
}}{}



\title{Unembedded Incremetalized Calculus: Compositional Approach to Cache-Transfer Style}
\author{Kazutaka Matsuda}


\begin{document}

\maketitle 


\section{Motivation}

\begin{itemize}
 \item \citet{CaiGRO14} is concise and elegant but has some practical limitation. 
 \item \citet{GiarrussoRS19} addresses the issue but this is a whole program transformation and 
   no discussions on type preservation. 
 \item We address the issues by designing an unembedded~\cite{AtLY09,Atkey09} DSL. 
\end{itemize} 

\section{Preliminaries}

\subsection{Unembedding}

Unembedding~\cite{AtLY09,Atkey09} is a transformation that converts tagless-final~\cite{CaKS09} style DSL into de Bruijn indexed style.\footnote{The same technique can be applied to PHOAS~\cite{Chlipala08}.}

Consider the following syntax for pure simply-typed $\lambda$-calculus in the tagless final style. 
\begin{code}
\=L \key{class}~\var{STLC} \A (e :: \con{Type} \to \con{Type}) ~\key{where}\\
\=L \quad {}
   \=M \var{lam} \=T :: (e \A a \to e \A b) \to e \A (a \to b) \\
   \=M \var{app} \=T :: e \A (a \to b) \to e \A a \to e \A b 
\end{code}
(For simplicity, we do not distinguish the types in Haskell (of kind $\con{Type}$) and the types in the target language.) 
The unembedding converts the above syntax into the following de Bruijn indexed first-order AST. 
\begin{code}
\=L \DATA~\con{In} \A (a :: \con{Type}) \A (\var{env} :: [\con{Type}])~\WHERE\\
\=L \quad {} 
  \=I \con{Z} \=t :: \con{In} \A a \A (a : \var{env}) \\
  \=I \con{S} \=t :: \con{In} \A a \A \var{env} \to \con{In} \A a \A (b : \var{env}) \\[\blanklineskip]
\=L \DATA~\con{D} \A (\var{env} :: [\con{Type}]) \A (a :: \con{Type}) \WHERE\\
  \=I \con{Var} \=T :: \con{In} \A a \A \var{env} \to \con{D} \A \var{env} \A a \\
  \=I \con{Lam} \=T :: \con{D} \A (a : \var{env}) \A b \to \con{D} \A \var{env} \A (a \to b) \\
  \=I \con{App} \=T :: \con{D} \A \var{env} \A (a \to b) \to \con{D} \A \var{env} \A a \to \con{D} \A \var{env} \A b 
\end{code}
The difficulty of this conversion is what argument we pass for $f$ of $\var{lam} \A f$. Basically, it must be $\con{Var} \A n$, 
but we cannot determine $n$ at this place: for example, 
for $\var{lam} \DOLLAR \lambda x \to x$, the $n$ must be $\con{Z}$, 
while, for $\var{lam} \DOLLAR \lambda x \to \var{lam} \DOLLAR \lambda y \to x$, the $n$ must be $\con{S} \A \con{Z}$, 
This would suggest that the interpretation must be parameterized by the current environment, as:
\begin{code}
\=L \key{data}~\con{Env} \A (f :: \con{k} \to \con{Type}) \A (\var{as} :: [k]) ~\WHERE \\
\=L \quad \con{ENil} :: \con{Env} \A f \A [\,] \\
\=L \quad \con{ECons} :: f \A a \to \con{Env} \A f \A \var{as} \to \con{Env} \A f \A (a : \var{as}) \\[\blanklineskip]%
\=L \key{type}~\con{SEnv} = \con{Env} \A \con{Proxy} \\[\blanklineskip]
\=L \key{newtype}~\con{U} \A a = \con{U} \A \{ \var{unU} :: \con{SEnv} \A \var{env} \to \con{D} \A \var{env} \A a) \}
\end{code}
This makes us to know the nesting depth of $\lambda$s. Especially, we can shift variables by comparing the depth. 
\begin{code}
\=L \var{diff} :: \con{SEnv} \A \var{env} \to \con{SEnv} \A \var{env'} \to \con{In} \A a \A \var{env} \to \con{In} \A a \A \var{env'}
\end{code}
This function is meaning full if the second environment is no smaller than the first one, and indeed partial. But, the unembedding guarantees 
that the use of $\var{diff}$ in the conversion below always succeeds. 
\begin{code}
\=L \key{instance}~\con{STLC} \A \con{U}~ \WHERE \\
\=L \quad {} 
 \=I \var{lam} \A f = \con{U} \DOLLAR \lambda \gamma \to \con{Lam} \DOLLAR  \\
 \=I \qquad {} 
      \=l \LET~\gamma_a = \con{ECons} \A \con{Proxy} \A \gamma \\
      \=l \IN~ \var{unU} \A (f \A (\con{U} \DOLLAR \lambda \gamma' \to \con{Var} \A (\var{diff} \A \gamma_a \A \gamma' \A \con{Z}))) \A \gamma_a\\
 \=I \var{app} = \dots \COMMENT{straightforward} \dots 
\end{code}

An advantage of the unembedding is that it enables us to advantage of both representations of syntax. Generally speaking, higher-order abstract syntax is easy to write because we do not care about shifting as in de Bruin indexed terms. On the other hand, HOAS is sometimes not suitable for program manipulation due to handling of functions such as $e \A a \to e \A b$ in $\var{lam}$. Especially in the tagless-final style, it is difficult to write a transformation that inspects transformation results, because the tagless-final style can be viewed as a build form of a term algebra. 

Another advantage of the unembedding is that we can implement the semantics in which $\sem{ \Gamma \vdash e : A }$ is not a function from $\sem{\Gamma}$ to $\sem{A}$ rather straightforwardly. An example of such semantics is backward evaluation~\cite{MatsudaW18haskell}, where $\sem{ \Gamma \vdash e : A } \in \sem{A} \to \sem{\Gamma}$, especially when we consider second-order language constructs, \ie, constructs that introduce variables such as $\CASE$ expressions, and (first-order) function abstractions.\footnote{Otherwise, we can just use Yoneda embedding to implement the semantics: 
$\sem{A} \to \sem{\Gamma}$ is isomorphic to $\forall s. (\sem{\Gamma} \to s) \to (\sem{A} \to s)$. The approach does not scale easily to second-order constructs, though. }

\subsection{Incrementalized $\lambda$ Calculus}

\newcommand{\NilChange}{{\bullet}}

\citet{CaiGRO14}'s incrementalized $\lambda$ calculus (ILC, for short) interprets $\lambda$ calculus so that it translates updates to avoid 
recomputation of results. An interesting point of the calculus is treatment of functions so that a system can have higher-order APIs such as $\var{map}$ and $\var{fold}$-like operations. 

More concretely, in ILC, a function $f$ of type $A \to B$ comes with its derivative 
$\partial f : A \to \Delta A \to \Delta B$, where $\Delta A$/$\Delta B$ denotes changes on $A$/$B$, satisfying: 
\[
 f \A (a \oplus \var{da}) = f \A a \oplus \partial f \A a \A \var{da} 
\]
Here, we write by $a \oplus \var{da}$ the result of applying update $\var{da} : \Delta A$ to a value $a : A$. 
In the calculus, a function of type $A \to B$ is also updatable by a function update $\Delta (A \to B)$, where 
\[
  \Delta(A \to B) = A \to \Delta A \to \Delta B\text{.}
\]
A function $f$ and its update $\var{df}$ must satisfy the law: 
\[
  (f \oplus \var{df}) \A (a \oplus \var{da}) = f \A a \oplus \var{df} \A a \A \var{da} 
\]
Especially, when $\var{da} = \NilChange$, a unit of $\oplus$ called nil update, we have 
\(
(f \oplus \var{df}) \A a  = f \A a \oplus \var{df} \A a \A \NilChange\text{.}
\) 
In general, $\var{df} \A a \A \NilChange$ may not be a nil change, intuitively because a function update involves 
both a change on free variables in $f$ and a derivative after the reflection of the update to free variable. 
If the former information is an nil update, meaning that it does not change the function itself, a function 
update defines a derivative of $f$, which is obtained from the above equation by with $(f \oplus \var{df}) = f$. 
Notice that this also means that a nil update on $A$ may not be unique or may not be obtained out of the thin air. 

The original semantics involves one that interprets a term as a derivative for a standard interpretation, \ie, 
$\sem{ \Gamma \vdash e : A } \in \sem{\Gamma} \to \Delta \sem{\Gamma} \to \Delta \sem{A}$. One might think that this 
would satisfy the criteria for which interpretation via unembedding will be useful. Some, however, would also notice
that we can tuple~\cite{HuITT97,Chin93} the semantics with the standard interpretation to obtain a tupled semantics; 
$\sem{ \Gamma \vdash e : A }^\mathrm{t} \in (\sem{\Gamma} \x \Delta \sem{\Gamma}) \to (A \x \Delta \sem{A})$. 
Notice that 
\[
 (A \to B) \times \Delta (A \to B) 
 =  (A \to B) \times (A \to \Delta A \to \Delta B) 
 \subseteq (A \x \Delta A) \to (B \x \Delta B)
\]
so there is no problem on the treatment of function values. 

The tupled semantics, however, is almost useless, because it does not support a typical use case of incremental computation: 
after an initial run, updates on the input can be translated to updates on the output, where we do not know what updates will come at
the initial run. 
The recomputation of the original values cannot be unavoidable in the tupled semantics.
\kztk{Should we move the following?}
Morihata \todo{his WPTE 2020 paper} addressed this issue by using Yoneda embedding\footnote{The original presentation does not mention Yoneda embedding. This explanation of his idea via Yoneda is our original.} to represent $\Delta A \to \Delta B$ as $(s \to \Delta A) \to s \to \Delta B$ where $s$ is abstract, 
so that we can obtain $\sem{ \Gamma \vdash e : A }^\mathrm{t} \in \forall s.\, \sem{\Gamma} \x (s \to \Delta \sem{\Gamma}) \to \sem{A} \x (s \to \Delta \sem{A})$, where abstract $s$ is universally quantified. 
In this approach, we can pass $\var{id} : \Delta \sem{\Gamma} \to \Delta \sem{\Gamma}$ in the initial run to obtain the update translator $(\Delta \sem{\Gamma} \to \Delta \sem{A})$ that can be 
used to translate a next single update on the input to one on the output. This approach can easily extended to consecutive translations of updates easily.
However, the functional representation means that the computation cannot be shared for multiple uses of variables. 
% One would think that this issue can be addressed by explicit sharing (\ie, \key{let}s). To implement a such construct, we need to give its semantics $\var{let}$ as:
% \[
% \ninfer{\var{let}}
% {
%   \sem{\Gamma} \x (s \to \Delta \sem{\Gamma}) \to \sem{B} \x (s \to \Delta \sem{B}) 
% }
% {
%   \sem{\Gamma} \x \sem{A} \x (s \to \Delta \sem{\Gamma} \x \Delta \sem{A}) \to \sem{B} \x (s \to \Delta \sem{B}) 
%   \qquad 
%   \sem{\Gamma} \x (s \to \sem{\Gamma}) \to \sem{A} \x (s \to \Delta \sem{A})
% }
% \]


Another issue of the original ILC is that they cannot support separation of updates. It is rather common that updates $\var{da} : \Delta A$
is represented by compositions of atomic updates $\var{da} = \var{da}_1 \oplus \dots \oplus \var{da}_n$.
% \footnote{
% Note that we can always find a composition operator by using Cayley representations. Let us write $\Delta_\mathrm{orig} A$ for 
% the original delta type that only supports applications $(\oplus) : A \to \Delta_\mathrm{orig} \to A$. Then, we can (re)define $\Delta A$ as $A \to A$
% where $a \oplus \var{da} = \var{da} \A a$, $\var{da}_1 \oplus \var{da}_2 = \var{da}_2 \circ \var{da}_1$, and the original delta $\var{da}_\mathrm{orig}$ is lifted as $\lambda x.(x \oplus \var{da}_\mathrm{orig})$.
% } 
(Here, we abuse the notation to use $\oplus$ both for composition and application.) 
A derivative is then expected to handle such atomic updates one by one, meaning that the equation 
\[
\partial f \A a \A (\var{da}_1 \oplus \var{da}_2) = \partial f \A a \A \var{da}_1 \oplus \partial f \A (a \oplus \var{da}_1) \A \var{da}_2 \tag{\textsc{DistILC}}\label{eq:dist-ilc}
\]
is expected to hold. However, this equation does not always hold for function updates, as they may not be derivatives. The \var{map} API for sequences discussed in 
\citet{GiarrussoRS19} is a good example in which this non-distribution becomes problematic. The API has type $(a \to b) \to \con{Seq} \A a \to \con{Seq} \A b$, 
and thus its incrementalized version should have type $(a \to b) \to (a \to \Delta a \to \Delta b) \to \con{Seq} \A a \to \Delta (\con{Seq} \A a) \to \Delta (\con{Seq} \A b)$. 
Here, $\Delta (\con{Seq} \A a)$ is represented as a list\footnote{Difference lists are used in the implementation of \citet{GiarrussoRS19} for performance.} of atomic changes, which include 
updates on single elements. We write such an update on the $i$th element as \(\con{At} \A i \A \var{da}\). 
Suppose that the incrementalized version of \var{map} received $[\con{At} \A i \A \var{da}_i]$. Then, one might think it is sufficient to call the given function update $\var{df}$ to the element $a_i$ of corresponding position $i$ to yield $[\con{At} \A i \A (\var{df} \A a_i \A \var{da}_i)]$. But, this is correct only when $\var{df}$ is a derivative; otherwise, $\var{df}$ changes to the function itself. 
So, to handle such function changes we need to call $\var{df} \A a_j \A \NilChange$ for other elements, and the correct output is: 
\[
[ \con{At} \A 0 \A (\var{df} \A a_0 \A \NilChange), \dots, \con{At} \A (i-1) \A (\var{df} \A a_{i-1} \A \NilChange), \con{At} \A i \A (\var{df} \A a_i \A \var{da}_i), \con{At} \A (i+1) \A (\var{df} \A a_{i+1} \A \NilChange), \dots, \con{At} \A n \A (\var{df} \A a_n \A \NilChange) ]
\]
This result suggests that it is hard to translate updates $[\con{At} \A {i_1} \A \var{da}_{i_1}, \con{At} \A {i_2} \A \var{da}_{i_2},\dots]$ one by one, especially when some updates refer to the same position. 

\subsection{Cache-Transfer Style}

\citet{GiarrussoRS19} address the recomputation issue of the original ILC by using caches. For a function $f : A \to B$, an incrementalized function in their 
system has the following type.\footnote{In the original paper, the second function takes $A$ in addition. This, however, can be realized by taking $C = C' \x A$ and is not regarded essential for formalization in this paper.}
\[
  (A \to B \x C) \x (\Delta A \to C \to \Delta B \x C)
\]
Here, $C$ is a datatype called cache, which may be different for each computation (and thus cannot be determined solely by $A$ and $B$ in general). 
The type above would be self-explanatory: we first build a cache together with the result by using the first component, 
and then repeatedly translate updates on $A$ to ones on $B$ by using the second component. 
Notice that the cache-transfer-style supports the translation of multiple updates as: 
\begin{align*}
 \cts f \A (\var{da}_1 \oplus \var{da}_2) 
 & = (\oplus) \OpFMAP \cts f \A \var{da_1} \OpMULT \cts f \A \var{da}_2 \\
 & = \lambda c.\:
       \bbt \LET~(\var{db}_1,c_1) = \cts f \A \var{da}_1 \A c~\IN    \\
            \LET~(\var{db}_2,c_2) = \cts f \A \var{da}_2 \A c_1~\IN \\
       \IN~(\var{db}_1 \oplus \var{db}_2, c_2) 
       \ee
 \tag{\textsc{DistCTS}}\label{eq:dist-cts}
\end{align*}
Here, we used \con{Applicative} operators ($\OpFMAP$ and $\OpMULT$) as if the $C \to \Delta B \x C$ part represents a state monad. 
More precisely, $\cts f$ that respects the structure 


Cache is sometimes the original argument as the \var{div} function below. 
\begin{code}
\=L \var{div}_\mathrm{init} :: \mathbb{R} \x \mathbb{R} \to \mathbb{R} \x \mathbb{R} \x \mathbb{R}\\
\=L \var{div}_\mathrm{init} \A (x,y) = (x / y, (x, y))\\[\blanklineskip]%
\=L \cts {\var{div}} :: \Delta (\mathbb{R} \x \mathbb{R}) \to \mathbb{R} \x \mathbb{R} \to \Delta (\mathbb{R} \x \mathbb{R}) \x \mathbb{R} \x \mathbb{R}\\
\=L \cts {\var{div}} \A (\var{dx}, \var{dy}) \A (x,y) = (x + dx) / (y + dy) - x / y 
\end{code}
Here, we assumed $\Delta \mathbb{R} = \mathbb{R}$ with $a \oplus \var{da} = a + \var{da}$, and $\Delta (A \x B) = \Delta A \x \Delta B$. 
(We used $\mathbb{R}$ as we do not focus on the precision issues.)
Sometimes, it may be different from the original argument. For example, for $\var{concat} : \con{Seq} \A (\con{Seq} \A A) \to \con{Seq} \A A$, 
the corresponding cache is lengths of inner sequences of type $\con{Seq} \A \con{Int}$. 



% Having $A$ would suggest that recomputation must happen but it is actually avoided by storing intermediate $A$s in cache; 
% that is, they actually are handled as if the cache contains $A$. 
% \[
%  (A \to B \x (C,A)) \x (\Delta A \to (C, A) \to \Delta B \x (C, A))
% \]




\subsection{Recovering Distributivity for Function Changes}
\label{sec:distributivity-and-function-changes}

The accompanied implementation to \citet{GiarrussoRS19} also discusses 
the distributivity issue, which itself is orthogonal to the cache-transfer style.
The basic idea is to separate a function change to $f$ into a derivative of $f$ and a change to values of free variables in $f$, because 
the former one is distributive (in the sense of \ref{eq:dist-ilc} and \ref{eq:dist-cts}). 

Let us use the original formalization to explain the idea. 
Suppose that $\var{df} : A \to \Delta A \to \Delta B$ is separated into $\var{df}_0 : A \to \Delta B$ and $\partial f : A \to \Delta A \to \Delta B$, where 
$\partial f$ must be a derivative of the corresponding function, in a sense that $\var{df} \A a \A \var{da} = \var{df}_0 \A a \oplus \partial f \A a \A \var{da}$.
Then, the change to the function is defined accordingly by 
\[
 f \oplus (\var{df}_0, \partial f) = \lambda x. f \A x \oplus \var{df}_0 \A x
\]
where $\partial f$ is not used at all as derivatives are nil changes. Now we have 
\[
 \var{df} \A a \A (\var{da}_1 \oplus \var{da}_2) = \var{df}_0 \A a \oplus \partial f \A a \A \var{da}_1 \oplus \partial f \A (a \oplus \var{da}_1) \A \var{da}_2 
\]
So that we can translate updates one by one. For example, consider the \var{map} case mentioned earlier. 
Then, the translation result of $[\con{At} \A i \A \var{da}_i]$ is now:
\[
[ \con{At} \A 0 \A (\var{df}_0 \A a_0), \dots, \con{At} \A n \A (\var{df}_0 \A a_n) ] \oplus [\con{At} \A i \A (\partial f \A a_i \A \var{da}_i)]
\]
So, a sequence $[\con{At} \A {i_1} \A \var{da}_{i_1}, \con{At} \A {i_2} \A \var{da}_{i_2},\dots]$ of updates is translated to 
\[
[ \con{At} \A 0 \A (\var{df}_0 \A a_0), \dots, \con{At} \A n \A (\var{df}_0 \A a_n) ] \oplus [\con{At} \A i_1 \A (\partial f \A a^{(1)}_{i_i} \A \var{da}_{i_1}), \con{At} \A (\partial f \A a^{(2)}_{i_2} \A \var{da}_{i_2}), \dots]
\]
where $a^j_i$ is the $i$th element of the sequence after $[\con{At} \A {i_1} \A \var{da}_{i_1}, \con{At} \A {i_2} \A \var{da}_{i_2},\dots, \con{At} \A {i_j} \A \var{da}_{i_j}]$ has been reflected. 

For the cache-transfer system, a direct adaptation of the above idea is to use $\var{df}_0 : C \to \Delta B \x C$ and $\cts f : \Delta A \to C \to \Delta B \x C$.
This representation works but has an issue in performance. These functions are obtained from an incrementalized function for a $\lambda$-body term-in-context 
$(h_\mathrm{init}, \cts h) : (\Gamma \x A \to C) \x (\Delta \Gamma \x \Delta A \to C \to \Delta B \x C)$ (ignored $\sem{-}$ for simplicity) by 
$\var{df}_0 \A c = \cts h \A (\var{d\theta}, \NilChange) \A c$ and $\cts f \A \var{da} \A c= \cts h \A (\NilChange, \var{da}) \A c$---involving two calls of $\cts h$---while the separation is needed when 
we handle atomic updates. 
So, their implementation produces such functions in an on-demand fashion; this exposes both $\Gamma$ and $C$, meaning that functions produced in different places usually have different types. 
This will not be a problem in their original framework, as they assumed an untyped system. 
Though their proofs do not cover this treatment of function updates, we believe that this treatment cannot yield runtime type errors. 

\newcommand{\FunObj}[2]{\con{Fun} \A #1 \A #2}

% \subsection{Notes on Function Objects}
% \label{sec:notes-on-function-objects}
% \todo{This discussion should not be in Preliminaries}

% We are expecting that $(\Delta A, \oplus, \NilChange)$ forms a monoid, but this does not apply to function changes, because $\oplus$ always ignores derivative information. 
% This may be caused by the fact that true function changes is only $A \to \Delta B$ and the derivative information is included just because of convenience to handle higher-order APIs (such as $\var{map}$).

% % Considering only having $\lambda x.e$ and $e_1 \A e_2$, in category theory suggest that a function type and its corresponding $\Delta$-type is those that makes
% % the following two types isomorphic.
% In category theory, function types, \ie, internal hom-sets are characterized by the monoidal closedness. In our context, it says that 
% a function type and its corresponding $\Delta$-type is those that makes
% the following two types isomorphic. 
% \begin{itemize}
%  \item 
%    \( (E \x A \to B ) \x ( E \x A \to \Delta E \x \Delta A \to \Delta B) \)
%  \item 
%    \( 
%    (E \to \FunObj{A}{B}) \x (E \to \Delta E \to \Delta (\FunObj{A}{B}))
%    \) 
% \end{itemize}
% A clear solution is to take $\con{Fun} \A A \A B = A \to B$ and $\Delta (\FunObj{A}{B}) = A \to \Delta A \to \Delta B$.
% Another choice is to make to compute the effect of having $E$ as soon as possible to pass $\NilChange :: \Delta E$ to the derivative. 
% Then, $\con{Fun} \A A \A B = (A \to B) \x (A \to \Delta A \to \Delta B)$ and 
% $\Delta (\con{Fun} \A A \A B) = A \to \Delta B$. 
% However, the conversion from the bottom to the top, because for an ordinary function the $E \to \con{Fun} \A A \A B$ part must be called with $\rho$ but for 
% an derivative it must be called with $\rho \oplus d\rho$. 


% For the cache transfer system, we instead consider an isomorphism between the following types. 
% \begin{itemize}
%  \item 
%    \( (E \x A \to B \x C ) \x ( \Delta E \x \Delta A \to C \to \Delta B \x C) \)
%  \item 
%    \( 
%       (E \to (\con{Fun} \A A \A B) \x C') \x (\Delta E \to C' \to \Delta (\con{Fun} \A A \A B) \x C')
%    \) 
% \end{itemize}
% This is actually harder than the previous one because of exposure of $C$; if we do not care about them, the solution corresponds to the above ``clear solution'' is 
% $\con{Fun} \A A \A B = A \to B \x C$ and $\Delta (\con{Fun} \A A \A B) = \Delta A \to C \to \Delta B \x C$. This is one of the reasons why \citet{GiarrussoRS19} considered an untyped system. 
% Similarly to the above discussion, we have a ``solution'' such that $\con{Fun} \A A \A B = (A \to B \x C) \x (\Delta A \to C \to \Delta B \x C)$, 
% $\Delta (\con{Fun} \A A \A B) = C \to \Delta B \x C$ and $C' = \UnitType$. 
% Ignoring the issue of exposure, there is no recomputation issue in the CTS, as the partially applied derivative is still valid as it does not depends on $E$ but $C$. 
% \todo{Is this really an isomorphism?}
% However, if we do not care about the exposure, another simple and effective solution exists: 
% $\con{Fun} \A A \A B = E \x (E \x A \to B \x C) \x ( \Delta E \x \Delta A \to C \to \Delta B \x C)$ and 
% $\Delta (\con{Fun} \A A \A B) = \Delta E$.
% \todo{Again, is this really a solution?}

% A solution is to store the derivative information to the ordinary transformation part. That is, we define the semantics of type as:
% \begin{align*}
%  \sem{ A \to B }        &= (\sem{A} \to \sem{B}) \x (\sem{A} \to \Delta \sem{A} \to \Delta \sem{B}) \\
%  \Delta \sem{ A \to B}  &= \sem{A}  \to \Delta \sem{B} 
% \end{align*}
% and define the semantics as:
% \begin{align*}
%  \sem{ \Gamma \vdash e : A }          &\in \sem{\Gamma} \to \sem{A} \\ 
%  \Delta \sem{ \Gamma \vdash e : A }  &\in \sem{\Gamma} \to \Delta \sem{\Gamma} \to \Delta \sem{A} 
%   \\
%  \sem{ \Gamma \vdash x : A} \A \rho &= \rho(x) 
%   \\
%  \sem{ \Gamma \vdash \lambda x. e : A \to B} \A \rho &= 
%    (\lambda a. \sem{ \Gamma, x : A \vdash e : B } \A (\rho, a), 
%     \lambda a. \lambda \var{da}.\: 
%       \Delta \sem{ \Gamma, x : A \vdash e : B } \A (\rho, a) \A (\NilChange, \var{da}))
%   \\
%  \sem{ \Gamma \vdash e_1 \A e_2 : B} \A \rho &= f \A a                                                                                                 
%    \qquad \WHERE~
%      \bbt
%        a &= \sem{ \Gamma \vdash e_2 : A } \A \rho      \\
%        (f, \dontcare) &= \sem{ \Gamma \vdash e_1 : A \to B } \A \rho 
%      \ee
%   \\
%   \Delta \sem{ \Gamma \vdash x : A} \A \rho \A \var{d\rho} &= \var{d\rho}(x) 
%   \\
%   \Delta \sem{ \Gamma \vdash \lambda x. e : A \to B} \A \rho \A \var{d\rho} 
%   &= 
%     \lambda a. \Delta \sem{ \Gamma, x : A \vdash e : B} \A (\rho, a) \A (\var{d\rho}, \NilChange) 
%   \\
%   \Delta \sem{ \Gamma \vdash e_1 \A e_2 : B } \A \rho \A \var{d\rho} 
%   &= \var{df} \A a \oplus \partial f \A a \A \var{da} 
%     \qquad \WHERE~
%       \bbt
%             a &= \sem{ \Gamma \vdash e_2 : A } \A \rho \\
%      \var{da} &= \Delta \sem{ \Gamma \vdash e_2 : A} \A \rho \A \var{d\rho} \\
%      \var{df} &= \Delta \sem{ \Gamma \vdash e_1 : A \to B } \A \rho \A \var{d\rho} \\
%      (\dontcare, \partial f) &= \sem{ \Gamma \vdash e_1 : A \to B} \A (\rho \oplus \var{d\rho}) 
%      \ee
% \end{align*}

% Some apparent inefficiency in the above definition would disappear in the cache-transfer style, while there is another issue 
% in handling function updates: cache must be exposed as 
% \begin{align*}
%  \sem{ A \to B }        &= (\sem{A} \to \sem{B} \x C) \x (\Delta \sem{A} \to C \to \Delta \sem{B} \x C) \\
%  \Delta \sem{ A \to B}  &= C \to \Delta \sem{B} \x C 
% \end{align*}

\subsection{Representation using Recursive Types}

We are able to hide the cache type by using a recursive type.
Notice that we have:
\[
  \exists C. (A \to B \x C) \x (\Delta A \to C \to \Delta B \x C)
  \subseteq 
   A \to B \x \exists C. (C \x (C \to (\Delta A \to \Delta B \x C)))
\]
By using the fact that $\nu F$ is isomorphic to $\exists C. C \x (C \to F \A C)$, we can use the recursive type 
\begin{code}
\=L \key{newtype}~\Interact{a}{b} = \con{Interact}\A \{ \var{interact} :: a \to (b, \Interact{a}{b}) \}
\end{code}
So, now we can write an incrementalized function between $A$ and $B$ as: 
\[
  A \to B \x (\Interact{\Delta A}{\Delta B})
\]
Here, $\Interact{\Delta A}{\Delta B}$ will be called derivative if it respects the monoid structure, that is: 
\begin{align*}
 \var{interact} \A i \A \NilChange &= (\NilChange, i) \\
 \var{interact} \A i \A (\var{da}_1 \oplus \var{da}_2) &= 
  \bbt \LET~(\var{db}_1,i_1) = \var{interact} \A i \A \var{da}_1~\IN\\
       \LET~(\var{db}_2,i_2) = \var{interact} \A i_1 \A \var{da}_2~\IN\\
       \quad (\var{db}_1 \oplus \var{db}_2, i_2) 
  \ee  
\end{align*}
We may think that $i : \Interact{\Delta A}{\Delta B}$ as a cache, where the corresponding delta translator is $\var{interact}$ (the order of arguments differs though). 
So, the above inclusion is actually an isomorphism. 
% Incrementalized functions $f$ also satisfy the following property. 
% \begin{align*}
% f \A (a \oplus \var{da}) = 
%   \bbt \LET~(b, i) = f \A a~\IN \\
%        \LET~(\var{db}, i') = \var{interact} \A i \A \var{da} \\
%   \quad  (b \oplus \var{db}, i') 
%   \ee
% \end{align*}


% Then, what a function change should be with the representation? One thing to care is we consider a change to an incrementalized function, rather than an ordinary function, to avoid 
% the monoid issue. 
% Of course, following the view point that $i : \Interact{\Delta A}{\Delta B}$ serves as a cache, we can follow the cache transfer system to 
% have $\Interact{\Delta A}{\Delta B} \to \Delta B \x (\Interact{\Delta A}{\Delta B})$, but here we are interested in a more direct, ``native'' solution.
% Following the original idea, function changes are reactions of $\Interact{\Delta A}{\Delta B}$ to nil changes. Then, what are such reactions? 
% Here, we introduce an important assumption that $i :: \Interact{\Delta A}{\Delta B}$ eventually becomes a derivative after translating $\NilChange$. That is, for a sequence 
% \begin{align*}
%    (\var{db}_1, i_1) &= \var{interact} \A i \A \NilChange\\
%    (\var{db}_2, i_2) &= \var{interact} \A i_1 \A \NilChange\\
%    (\var{db}_3, i_3) &= \var{interact} \A i_2 \A \NilChange \\
%                      & \vdots 
% \end{align*}
% there is some $i_k$ that is a derivative. If we have such a way to find $i_k$ and $\var{db}_1 \oplus \dots \oplus \var{db}_n$, this squeezing process 
% indeed is a function change. But, who captures the changes to free variables in a closure then? 

Then, what are function changes? Recall that a function $\con{Fun} \A A \A B$ and function update $\Delta (\con{Fun} \A A \A B)$ are those that make the following two types isomorphic.
\begin{itemize}
 \item \( E \x A \to B \x (\Interact{(\Delta E \x \Delta A)}{\Delta B}) \)
 \item \( E \to (\con{Fun} \A A \A B) \x (\Interact{\Delta E}{\Delta (\con{Fun} \A A \A B)}) \)
\end{itemize}
If we were allowed exposure of $E$, the solution would exist, as: 
$\con{Fun} \A A \A B = A \to B \x (\Interact{(\Delta E \x \Delta A)}{\Delta B})$
and 
$\Delta (\con{Fun} \A A \A B) = \Delta E$. 
Considering this $\Delta E$ can only be used as an argument of $\var{interact} \A (i : \Interact{(\Delta E \x \Delta A)}{\Delta B})$ to produce $\Delta B$ and $i' : \Interact{(\Delta E \x \Delta A)}{\Delta B}$, 
this ``solution'' is nothing but taking $C = \Interact{(\Delta E \x \Delta A)}{\Delta B}$. 


% \begin{hscode}
% l2r :: ((e, a) -> (b, Interact (Delta e, Delta a) (Delta b)))
%        -> (e -> (a -> (b, Interact (Delta a) (Delta b)), Interact (Delta e) (a -> Delta b)))
% l2r h e = (\a -> let (b, i) = h (e, a) 
%                  in (b, nil2fst i), 
           
% \end{hscode}


% Then, we can define $\oplus$ for functions as: 
% \[
%    (f \oplus \var{df}) \A a = f \A a \oplus \pi_1 \A (\var{df} \A 
% \]
% However, this has a similar problem of not being a monoid as it contains the information of derivatives. 
% Similarly to the above discussions we can think a reaction to $\NilChange$ as a function update, \ie, $A \to \Delta B \x (\Interact{\Delta A}{\Delta B})$. 


\section{Unembedding Cache-Transfer System}

\subsection{Embedding of First-Order Fragments}
If we ignore function values and updates for a while, 
it is rather easy to define the CTS semantics of terms-in-context as: 
\begin{align*}
  {\sem{ \Gamma \vdash e : A }} 
  &\in \sem{\Gamma \vdash - : A} =  \exists C.\: (\sem{\Gamma} \to \sem{A} \x C) \x (\Delta \sem{\Gamma} \to C \to \Delta \sem{A} \x C)
  \\
  \sem{ \Gamma \vdash x : A } 
  &= \UnitType, \lambda \rho. (\rho(x), \UnitValue), \lambda d\rho. \lambda c. \, (d\rho(x), c)
  \\
  \sem{ \Gamma \vdash (e_1,e_2) : A_1 \x A_2 } 
  &= \var{pairSem} \A \sem{ \Gamma \vdash e_1 : A_1 } \A \sem{\Gamma \vdash e_2 : A_2}
  \\
  \sem{ \Gamma \vdash \ms{op} \A e : B} 
  &= \var{opSem}_{\ms{op} : A \to B} \A \sem{ \Gamma \vdash e : A }
\end{align*}
where 
\begin{alignat*}{3}
&\var{pairSem} : \sem{ \Gamma \vdash - : A_1 } \to \sem{ \Gamma \vdash - : A_2 } \to \sem{ \Gamma \vdash - : A_1 \x A_2 } \\
&\var{pairSem} \A (C_1,\init{f_1},\cts{f_1}) \A (C_2,\init{f_2}, \cts{f_2})  = C_1 \x C_2, \init{h}, \cts{h} 
\\
& \quad {\WHERE~
     \bbt 
        \init{h} \A \rho = \LET~\{(a_i,c_i) = \init{f_i} \A \rho\}_i~\IN~((a_1,a_2),(c_1,c_2))\\
        \cts{h} \A d\rho \A (c_1,c_2) = \LET~\{ (\var{da}_1,c_1') = \cts{f_i} \A d\rho \A c_i \}_i~\IN~((\var{da}_1,\var{da}_2),(c'_1,c'_2))
     \ee}\\
&\var{opSem}_\ms{op : A \to B} : \sem{ \Gamma \vdash - : A} \to \sem{ \Gamma \vdash - : B}\\
&\var{opSem}_\ms{op : A \to B} \A (C, \init{f}, \cts{f}) = C \x C_{\ms{op}} , \init{h}, \cts{h} \\
& \quad \WHERE~
      \bbt
        \init{h} \A \rho = \LET~\{(a, c) = \init{f} \A \rho; (b, c_{\ms{op}}) = \init{\ms{op}}\} \A a~\IN~ (b, (c, c_\ms{op})) \\
        \cts{h} \A d\rho \A (c,c_\ms{op}) = \LET~\{ (\var{da}, c') = \cts{f} \A d\rho \A c; (\var{db}, c'_\ms{op}) = \cts{\ms{op}} \A \var{da} \A c_\ms{op} \}~\IN~(\var{db}, (c', c'_\ms{op}))
      \ee 
\end{alignat*}
Here, we used the pair constructor whose first component is a type to represent existential types as in Agda.
Roughly speaking, $\ms{op} : A \to B$ here represents a first-order API such as $\var{concat}$ and $\var{div}$, which 
consists of a cache type $C_\ms{op}$, an initializer $\init{\ms{op}} : A \to B \x C_\ms{op}$ and an update translator $\cts{\ms{op}} : \Delta A \x C_\ms{op} \to \Delta B \x C_\ms{op}$. 
Such APIs can be $n$-ary; arguments for such operators can be constructed via $\var{pair}$. 


In addition to the constructs above, we also consider $\LET$s for explicit sharing, which is also useful for incrementalized computation. 
\[\bb
\sem{ \Gamma \vdash \LET~x = e_1~\IN~e_2 : B }
= \var{letSem} \A \sem{ \Gamma \vdash e_1 : A } \A \sem{ \Gamma, x : A \vdash e_2 : B}\\[\blanklineskip]%
\var{letSem} : \sem{ \Gamma \vdash - : A } \to \sem{ \Gamma, x : A \vdash - : B } \to \sem{ \Gamma \vdash : B }\\
\var{letSem} \A (C_1, \init{f_1}, \cts{f_1}) \A (C_2, \init{f_2}, \cts{f_2}) =
C_1 \x C_2, \init{h}, \cts{h} \\
\quad \WHERE~
 \bbt
    \init{h} \A \rho = \LET~(a, c_1) = \init{f_1} \A \rho ; (b, c_2) = \init{f_2} \A (\rho, a)~\IN~(b, (c_1,c_2))\\
    \cts{h} \A \var{d\rho} \A (c_1,c_2) = \LET~(\var{da}, c_1') = \cts{f_1} \A d\rho \A c_1; (\var{db}, c_2') = \cts{f_2} \A (d\rho, \var{da}) \A c_2~\IN~(\var{db}, (c_1', c_2'))
 \ee
\ee\]

This is a typical example that embedding via unembedding~\cite{Atkey09, AtLY09} is useful. 
Preparing a type class that represents a syntax 
\begin{code}
\=L \key{data}~\key{family}~\Delta a = \dots \\
\=L \DATA~\con{CTS}\A a \A b~\WHERE ~ \LCOMMENT{GADT-style definition for existential types}\\
\=L \quad \con{CTS} :: (a \to (b, c)) \to (\Delta a \to c \to (\Delta b, c)) \to \con{CTS} \A a \A b \\[\blanklineskip]%
\=L \CLASS~\con{CTSExp} \A e~\WHERE \\
\=L \quad \var{pair} :: e \A a \to e \A b \to e \A (a, b) \\
\=L \quad \var{lift} :: \con{CTS} \A a \A b \to e \A a \to e \A b \\
\=L \quad \var{share} :: e \A a \to (e \A a \to e \A b) \to e \A b 
\end{code}
we can define its instance as:
\begin{code}
\=L \key{newtype}~\con{UnCTS}~a = \con{UnCTS}~\{ \var{cts} :: \forall \var{env}.\, \con{SEnv} \A \var{env} \to \con{CTS} \A (\con{Env} \A \var{as}) \A a) \}\\
\=L \key{instance} \A \con{CTSExp} \A \con{UnCTS}~\WHERE~\dots \COMMENT{omitted as straightforward adaptation of unembedding}\dots
%\=L \quad \var{pair} \A e_1 \A e_2 = \con{UnCTS} \DOLLAR \lambda \gamma \to \var{pairSem} \A (\var{cts} \A e_1 \A \gamma) \A (\var{cts} \A e_2 \A \gamma)
\end{code}
Having HOAS (more precisely, the tagless final~\cite{CaKS09} here) interface not only relieve the syntactic pain of using de Bruijn indexed terms, 
but also we can use Haskell functions to construct abstract syntax trees more effectively. 
The sharing operation $\var{share}$ plays an important role here; it enables us to distinguish the host-level sharing (that copies ASTs and duplicates computation) from 
the guest-level explicit sharing (\ie, \key{let}).

\subsection{Embedding of Second-Order APIs}

One of the strength of the unembedding is that it can support embedding of the second-order language constructs~\todo{citation}, \ie, language constructs with binders such as $\key{let}$. 
For example, the $\var{map}$ API can be seen as a second-order language construct, as: 
\[
\infer
{
 \Gamma \vdash \var{map} \A (x.e_1) \A e_2 : \con{Seq} \A B 
}
{
 \Gamma, x : A \vdash e_1 : B 
 & 
 \Gamma \vdash e_2 : \con{Seq} \A A
}
\]
Accordingly, if we have a semantic function for the $\var{map}$ API as:
\[
\var{mapSem} : \sem{ \Gamma, x : a \vdash - : b } \to \sem{ \Gamma \vdash - : \con{Seq} \A a} \to  \sem{ \Gamma \vdash - : \con{Seq} \A b }
\]
we can extend the syntax 
\begin{code}
\=L \key{class}~\con{CTSExp} \A e \To \con{HasMap} \A e~\key{where}~\\
\=L \quad 
        \var{map} : (e \A a \to e \A b) \to e \A (\con{Seq} \A a) \to (\con{Seq} \A b) 
\end{code}
and give its implementation similarly to $\var{lam}$ in the previous section. 
\begin{code}
\=L \key{instance}~\con{HasMap} \A \con{UnCTS}~\WHERE~\dots 
\end{code}
We omit the concrete definition as it is a routine. 

In general, for a construct 
\[
\infer
{ 
 \Gamma \vdash \con{c} \A \{ \V{x}.e_i \}_{1 \le i \le n} : C
}
{
  \{ \Gamma, \V{x : A_i} : B_i \}_{1 \le i \le n}
}
\]
with a corresponding semantic function 
\[
\var{sem} : \sem{ \Gamma, \V{x : A_1} \vdash - : B_1} \to \dots \to \sem{ \Gamma, \V{x : A_n} \vdash - : B_n} \to \sem{ \Gamma \vdash - : C }
\]
we can have the following method in a class that represents the syntax. 
\begin{code}
\=L \key{class}~\dots \To \con{HasSyn} \A e~\WHERE\\
\=L \quad \var{syn} :: (\V{e \A A_1} \to e \A B_1) \to \dots \to (\V{e \A A_n} \to e \A B_n) \to e \A C 
\end{code}
Leveraging the type-level programming power of Haskell, we can prepare one method to lift all the semantics functions like $\var{sem}$, but in this presentation we 
insist on having individual methods such as $\var{map}$ for semantic functions. 

Now, let us discuss how we can implement $\var{mapSem}$, as it may share common patterns with other second-order APIs. 
An observation is that it is rather easier to 
implement the function 
\[
\var{mapSem}' : \sem{ \Gamma, x : A \vdash - : B } \to \sem{ \Gamma, y : \con{Seq} \A A \vdash \con{Seq} \A B }
\]
and define the $\var{mapSem}$ as $\var{mapSem} \A e_1 \A e_2 = \var{letSem} \A e_2 \A (\var{mapSem}' \A e_1)$
because it reduces the number of semantic objects to be handled from three to two. Even for $\con{CTS}$, this reduces a tedious 
manipulation of caches and existential quantifiers. But, this impact gets bigger when we consider more complex semantic domain in later. 

Then, let us focus on $\var{mapSem}'$ then. Here, the cache type for $\sem{ \Gamma, y : \con{Seq} \A a \vdash \con{Seq} \A b }$ 
is $(\con{Seq} \A C) \x \sem{\Gamma}$, a sequence of individual caches and the original environment, where $C$ is the cache type for 
$\sem{ \Gamma, x : a \vdash - : b }$. Then, the initializer's behavior is rather straightforward: it receives $(\rho, \var{as})$,
then defines $\init{h} = \lambda a.\init{f} \A (\rho,a)$ and maps $h$ to $\var{as}$ to produces the cache and the result. 
The delta translator does a bit more interesting thing. Receiving $(d\rho, \var{das})$, 
it then generate from $\cts{f}$ two functions $\var{dh}_0 = \cts{f} \A (d\rho, \NilChange)$ and 
$\cts{h} \A \var{da} = \cts{f} \A (\NilChange, \var{da})$, a function change and a delta translator corresponding to $h$. 
Then, it maps $\var{dh}_0$ to the sequence of individual caches to produce new cache sequence and updates to individual elements.
After that, it processes updates $\Delta (\con{Seq} \A a)$, which is represented as a list of atomic updates one by one using $\cts{h}$. 
Notice that we need to include $\sem{\Gamma}$ in the cache, because for insertions we need to call the original function, which requires $\rho :: \sem{\Gamma}$, to obtain the transformation results of inserted elements. 

We have the following observations from the behavior of $\var{mapSem}'$. 
\begin{itemize}
 \item It essentially handles a function object and corresponding update ($\init{h} : \sem{A} \to \sem{B} \x C$, $\var{dh}_0 : C \to \Delta \sem{B} \x C$, $\cts{h} : \Delta \sem{A} \to C \to \Delta \sem{B} \x C$).
 \item Storing $\Gamma$ as a cache is costly in space; it will take quadratic space for nested $\var{map}$, as each $\var{map}$ stores 
   an environment at the point. This is bad, as such caches contain a lot of overlaps. 
 \item How $\var{dh}$ will be used mirrors how $\init{h}$ is used. So, we may be able to share some patterns. 
 \item The treatment of $\cts{h}$ looks problem specific, and would be hard to share. 
\end{itemize}


\subsection{Cache-Size Reduction Especially for Recomputation}
\label{sec:cache-size-optimized-version}

\newcommand{\orig}[1]{#1^\mathrm{o}}
\newcommand{\recomp}[1]{#1^\mathrm{r}}
\newcommand{\Restrict}[2]{#1|_{#2}}

Above, we have seen that we need to store entire $\sem{\Gamma}$ to implement the \var{map} API, which is costly and introduces redundancy in cache. For $\var{map} \A (\lambda x. \dots \var{map} \A (\dots) )$, it 
will store both $\sem{\Gamma}$ and $\sem{\Gamma, x : A}$---whole $\sem{\Gamma}$ is copied. This space inefficiency is addressed by using the following ideas. 
\begin{itemize}
\item We statically infer the variables used in an expression so that an initializer and a delta translator take a subpart of the environment needed for the computation. 
\item Also, we compositionally construct environments to store. Notice that such environments are needed only when recomputation is required in a delta translator, as in \var{map}. 
\end{itemize}
So, now a term-in-context $\Gamma \vdash e : A$ is interpreted as
\[
 \sem{\Gamma \vdash e : A } = \exists C. \exists \orig{U}. \exists \recomp{U}. 
  (\sem{ \Restrict{\Gamma}{\orig{U}}} \to \sem{A} \x C) \x 
  (\Delta \sem{ \Restrict{\Gamma}{\orig U} } \to \sem { \Restrict{\Gamma}{\recomp U} } \to C \to \Delta \sem{A} \x C)
\]
Here, $U$ is a set of variables that are used in a designated computation, and $\Restrict{\Gamma}{U}$ represents the extraction of the used part. 
When we combine the two computation, we need to merge uses, which is done by the following function.
\[
\var{mergeUses} : (U_1 : \con{Use}) \to (U_2 : \con{Use}) 
            \to \exists U. (\forall \Gamma. \sem{\Restrict{\Gamma}{U}} \to \sem{\Restrict{\Gamma}{U_1}}) 
                            \x  (\forall \Gamma. \sem{\Restrict{\Gamma}{U}} \to \sem{\Restrict{\Gamma}{U_2}}) 
\]
(For simplicity of presentation, we used Agda-like dependent products and sums here. In Haskell implementation, we will use corresponding singleton types.)
\[
\sem{ \Gamma \vdash x : A } = \UnitType, \{ x \}, \emptyset, (\lambda a. (a, \UnitValue)), (\lambda \var{da}.\lambda \dontcare.\lambda c.(\var{da}, c))
\]
Here, we abuse the notation to write a singleton environment as the element that it contains in the same way. 
\todo{This representation is not good as it does not work well with function-like objects. We may need to say that $C$ contains information of $\sem{\Restrict{\Gamma}{\recomp U}}$.}

For products and primitives, it suffices to redefine the corresponding semantic functions. 
\begin{alignat*}{3}
&\var{pairSem} : \sem{ \Gamma \vdash - : A_1 } \to \sem{ \Gamma \vdash - : A_2 } \to \sem{ \Gamma \vdash - : A_1 \x A_2 } \\
&\var{pairSem} \A (C_1, \orig{U_1}, \recomp{U_2}, \init{f_1},\cts{f_1}) \A (C_2, \orig{U_2}, \recomp{U_2},\init{f_2}, \cts{f_2})  = C_1 \x C_2, \orig{U}, \recomp{U}, \init{h}, \cts{h} 
\\
& \quad {\WHERE~
     \bbt 
        (\orig{U}, \orig{\var{ext}_1}, \orig{\var{ext}_2}) = \var{mergeUses} \A \orig{U_1} \A \orig{U_2}\\
        (\recomp{U}, \recomp{\var{ext}_1}, \recomp{\var{ext}_2}) = \var{mergeUses} \A \recomp{U_1} \A \recomp{U_2} \\
        \init{h} \A \rho = \LET~\{(a_i,c_i) = \init{f_i} \A (\orig{\var{ext}_i} \A \rho)\}_i~\IN~((a_1,a_2),(c_1,c_2))\\
        \cts{h} \A d\rho \A \rho \A (c_1,c_2) = \LET~\{ (\var{da}_1,c_1') = \cts{f_i} \A (\orig{\var{ext}_i} \A d\rho) \A (\recomp{\var{ext}_i} \A \rho) \A c_i \}_i~\IN~((\var{da}_1,\var{da}_2),(c'_1,c'_2))
     \ee}\\
&\var{opSem}_\ms{op : A \to B} : \sem{ \Gamma \vdash - : A} \to \sem{ \Gamma \vdash - : B}\\
&\var{opSem}_\ms{op : A \to B} \A (C, \orig{U}, \recomp{U}, \init{f}, \cts{f}) = C \x C_{\ms{op}} , \orig{U}, \recomp{U}, \init{h}, \cts{h} \\
& \quad \WHERE~
      \bbt
        \init{h} \A \rho = \LET~\{(a, c) = \init{f} \A \rho; (b, c_{\ms{op}}) = \init{\ms{op}}\} \A a~\IN~ (b, (c, c_\ms{op})) \\
        \cts{h} \A d\rho \A \rho \A  (c,c_\ms{op}) = \LET~\{ (\var{da}, c') = \cts{f} \A d\rho \A \rho \A c; (\var{db}, c'_\ms{op}) = \cts{\ms{op}} \A \var{da} \A c_\ms{op} \}~\IN~(\var{db}, (c', c'_\ms{op}))
      \ee 
\end{alignat*}
The definition of $\var{letSem}$ for this semantics is more interesting. 
\[\bb
\var{letSem} : \sem{ \Gamma \vdash - : A } \to \sem{ \Gamma, x : A \vdash - : B } \to \sem{ \Gamma \vdash : B }\\
\var{letSem} \A (C_1, \orig{U_1}, \recomp{U_1}, \init{f_1}, \cts{f_1}) \A (C_2, \orig{U_2}, \recomp{U_2}, \init{f_2}, \cts{f_2}) =
C, \orig{U}, \recomp{U}, \init{h}, \cts{h} \\
\quad \WHERE~
 \bbt
    C = \begin{cases} 
      C_1 \x C_2 \x A &\text{if}~x \in \recomp{U_2} \\
      C_1 \x C_2      &\text{otherwise} 
      \end{cases}\\
    (\orig{U}, \orig{\var{ext}_1}, \orig{\var{ext}_2}) = \var{mergeUses} \A \orig{U_1} \A (\orig{U_2} \setminus \set{x})\\
    (\recomp{U}, \recomp{\var{ext}_1}, \recomp{\var{ext}_2}) = \var{mergeUses} \A \recomp{U_1} \A (\recomp{U_2} \setminus \set{x})\\
    \init{h} \A \rho = 
      \bbt \LET~(a, c_1) = \init{f_1} \A (\orig{\var{ext}_1} \A \rho) ~\IN\\
           \LET~(b, c_2) = \init{f_2} \A (\var{extend} \A x \A \orig{U_2} \A a \A (\orig{\var{ext}_2})) \A \rho)~\IN\\ 
      \quad \IF~x \in \recomp{U_2}~\THEN~(b, (c_1,c_2,a))~\ELSE~(b,(c_1,c_2))
      \ee \\
    \cts{h} \A \var{d\rho} \A \rho = 
      \begin{cases}
        \lambda (c_1,c_2, a). 
          \bbt 
          \LET~(\var{da}, c_1') = \cts{f_1} \A (\orig{\var{ext}_1} \A d\rho) \A (\recomp{\var{ext}_1} \A \rho) \A c_1~\IN\\
          \LET~a' = a \oplus \var{da} \\
          \LET~(\var{db}, c_2') = \cts{f_2} \A (\var{extend} \A x \A \orig{U_2} \A \var{da} \A (\orig{\var{ext}_2} \A d\rho)) \A (\recomp{\var{ext}_2} \A \rho, a')  \A c_2~\IN\\
          \quad (\var{db}, (c_1', c_2',a'))
          \ee
        & \text{if}~x \in \recomp{U_2} \\
        \lambda (c_1,c_2). 
          \bbt 
          \LET~(\var{da}, c_1') = \cts{f_1} \A (\orig{\var{ext}_1} \A d\rho) \A (\recomp{\var{ext}_1} \A \rho) \A c_1~\IN\\
          \LET~(\var{db}, c_2') = \cts{f_2} \A (\var{extend} \A x \A \orig{U_2} \A \var{da} \A (\orig{\var{ext}_2} \A d\rho)) \A (\recomp{\var{ext}_2} \A \rho)  \A c_2~\IN\\
          \quad (\var{db}, (c_1', c_2'))
          \ee
        & \text{otherwise}
      \end{cases}               
 \ee
\ee\]
Here, we only store the value of $x$ only if it is needed. We store an updated element instead of the original one, because the 
$\rho$ part passed to a delta-translator is supposed to be used in derivatives, which are called after changes to environments has been reflected. 

Also, we can avoid the issue of cache size by using the identity $C \x \UnitType = \UnitType \x C = C$. Addressing this is rather straightforward (e.g., we can use join list representations for $C$) 
and thus the discussions about it are  omitted in this manuscript. 

Notice that manipulation of uses introduces quite a few overhead. For example, $\var{mergeUses}$ takes time proportional to the size of uses, when uses are represented as lists or vectors. 
Also, extraction (obtained by $\var{mergeUses}$) takes time proportional to sizes and uses. 
Currently, we use staged computation to make this overhead will not happen in the incrementalized computation itself. 

\subsection{Function-Like Objects}


What is a function object? In category theory, it is characterized via monoidal closedness: that is, a function object (formally, an internal hom-set object) $\FunObj{A}{B}$ 
in an object that establishes the following natural isomorphism. 
\[
\mathcal{C} \A (E \x A) \A B \simeq \mathcal{C} \A E \A (\FunObj{A}{B})
\]
In addition to this, for our case, we need to ensure that $\Delta (\FunObj{A}{B})$ represents 
changes, meaning that $\Delta (\FunObj{A}{B})$ must form a monoid and that there is some $(\oplus) : \FunObj{A}{B} \to \Delta (\FunObj{A}{B}) \to \FunObj{A}{B}$ that respect the monoid structure of $\Delta (\FunObj{A}{B})$. 

In the original calculus by \citet{CaiGRO14}, where 
\[
\mathcal{C} \A E \A A = (E \to A) \x (E \to \Delta E \to \Delta A)
\]
a function object is to make the following types isomorphic. 
\begin{itemize}
 \item 
   \( (E \x A \to B ) \x ( E \x A \to \Delta E \x \Delta A \to \Delta B) \)
 \item 
   \( 
   (E \to \FunObj{A}{B}) \x (E \to \Delta E \to \Delta (\FunObj{A}{B}))
   \) 
\end{itemize}
A clear solution is to take $\con{Fun} \A A \A B = A \to B$ and $\Delta (\con{Fun} \A A \A B) = A \to \Delta A \to \Delta B$. 
This is good when we interpret $\lambda$ terms with first-order primitives, but has a practical limitation for handling 
second-order primitives such as the $\var{map}$ API (Section~\ref{sec:distributivity-and-function-changes}), because $\Delta(\FunObj{A}{B})$ may not be a monoid.
% language constructs with binders such as $\key{let}$, $\lambda$, and domain-specific APIs such the $\var{map}$ API for sequences. 


So, we want to separate a function change into a change to the function itself and a derivative of the function. This idea is represented by a solution: 
$\FunObj{A}{B} = (A \to B) \x (A \to \Delta A \to \Delta B)$ and $\Delta (\FunObj{A}{B}) = A \to \Delta B$.
For this to be an answer, we need to leverage the property that $\mathcal{C} \A A \A B$ and $\FunObj{A}{B}$ consist of pairs of 
a function and its derivative. 


For the original CTS, where $\mathcal{C} \A A \A B = \exists C. (A \to B \x C) \x (\Delta A \to C \to \Delta B \x C)$, 
a function object $\FunObj{A}{B}$ makes the following two types isomorphic. 
\begin{itemize}
 \item 
   \( (E \x A \to B \x C ) \x ( \Delta E \x \Delta A \to C \to \Delta B \x C) \)
 \item 
   \( 
      (E \to (\FunObj{A}{B}) \x C') \x (\Delta E \to C' \to \Delta (\FunObj{A}{B}) \x C')
   \) 
\end{itemize}
Finding a solution for this case is actually harder than the previous one, because 
$\FunObj{A}{B}$ cannot depend on other types than $A$ and $B$. 
This is one of the reasons why \citet{GiarrussoRS19} considered an untyped system, 
and we also doubt that there exists a meaningful solution. So, let us forget about the 
restriction for now and allowing $\FunObj{A}{B}$ (and $\Delta (\FunObj{A}{B})$) to contain $C$.
A corresponding solution to the above one is: 
$\FunObj{A}{B} = (A \to B \x C) \x (\Delta A \to C \to \Delta B \x C)$, 
$\Delta (\FunObj{A}{B}) = C \to \Delta B \x C$ and $C' = \UnitType$. 
\todo{I don't know whether this actually defines isomorphism. Check it.}

In \citet{GiarrussoRS19}, they consider closures and changes to them. So, they exposes $E$ as: $\FunObj{A}{B} = E \x (E \to A \to B \x C)$ and 
$\Delta (\FunObj{A}{B}) = \Delta E \x (\Delta E \x \Delta A \to C \to \Delta B \x C)$.
Similarly to the above discussion, to avoid the monoid issue, we also 
have a solution in which $\FunObj{A}{B} =  E \x (E \to A \to B \x C) \x  (\Delta E \x \Delta A \to C \to \Delta B \x C)$ and $\Delta (\FunObj{A}{B}) = \Delta E$.
Now we are able to hide $C$ but now need to expose $E$. Notice that this $E$ need not be the same as $E$ in the outside of $\FunObj{A}{B}$ ad $\Delta (\FunObj{A}{B})$.


The above discussions suggest that we have a pseudo function object with functions, such as:
\[
\bb
  \var{pLam}  &:: \mathcal{C} \A \sem{\Gamma, x : A} \A \sem{B} \to \exists C. \mathcal{C} \A \sem{\Gamma} \A (\con{Fun}_C \A A \A B) \\
  \var{pApp}  &:: \mathcal{C} \A ((\con{Fun}_C \A A \A B) \x A) \A B \\
%  \var{split} &:: (\con{Fun}_C \A A \A B) \x \Delta (\con{Fun}_C \A A \to B) \to 
%    \exists C. (A \to B \x C) \x (C \to \Delta B \x C) \x (\Delta A \to C \to \Delta B \x C)
\ee               
\]
For example, if we take the former representation (which exposes $C$), these functions can be implemented as below (again, we use an Agda-like syntax for handling existential quantifiers).
\begin{code}
\=L \var{pLam} :: \mathcal{C} \A (E \x A) \A B \to \exists C. \mathcal{C} \A E \A (\con{Fun}_C \A A \A B)\\
\=L \var{pLam} \A (C, h, \var{dh}) = C,\, \UnitType,\, \lambda e.(h \A e, \lambda \var{da}. \var{dh} \A (\NilChange, \var{da})),\, \lambda \var{de}.\var{dh} \A (\var{de}, \NilChange)\\[\blanklineskip]%
\=L \var{pApp} :: \mathcal{C} \A ((\con{Fun}_C \A A \A B) \x A) \A B\\
\=L \var{pApp} = (\Delta A \to C \to \Delta B \x C) \x C ,\, h ,\, \var{dh} \\
\=L \quad {} \WHERE~
 \=W h \A ((f, \partial f), a) = \LET~(b,c) = f \A a~\IN~(b, (\partial f, c)) \\
 \=W \var{dh} \A (\var{df}_0 , \var{da}) \A (\partial f, c) = {}
      \=l \LET~(\var{db}_1, c') = \var{df}_0 \A c~\IN \\
      \=l \LET~(\var{db}_2, c'') = \partial f \A \var{da} \A c'~\IN \\
      \=l \quad (\var{db}_1 \oplus \var{db}_2, (\partial f, c'')) 
\end{code}
Now, we are ready to define the $\var{map}$ API via
\[
 \var{pMap} :: \forall C. \forall A. \forall B. \exists C'. \mathcal{C} \A (\con{Fun}_C \A A \A B) \A (\con{Fun}_{C'} \A (\con{Seq} \A A) \A (\con{Seq} \A B))
\] 
which makes us free from handling of environments ($\Gamma$).
\todo{How can we define $\var{pMap}$ so that the implementations are common to both representations of $\FunObj{A}{B}$?}


\todo{We have not checked the following discussions by an implementation.}
Now, we discuss how this discussion applies to the cache optimized version in Section~\ref{sec:cache-size-optimized-version}.
One thing to care here is how we abstract uses ($U$), which are syntactic notions and thus appeared inside the semantic brackets. Here, we simply assume that there 
are objects $\Restrict{E}{U}$ corresponding to $\sem{\Restrict{\Gamma}{U}}$ where $E = \sem{\Gamma}$.
So, in this case, we have 
\[
\mathcal{C} \A E \A A = \exists C. \exists \orig{U}. \exists \recomp{U}. 
  (\Restrict{E}{\orig{U}} \to {A} \x C) \x 
  (\Delta \Restrict{E}{\orig U} \to \Restrict{E}{\recomp U} \to C \to \Delta A \x C)
\]
and a corresponding function object must make the following types isomorphic. 
\begin{itemize}
 \item \(
   \exists \orig{U}. \exists \recomp{U}. 
   (\Restrict{(E \x A)}{\orig{U}} \to {B} \x C) \x 
   (\Delta \Restrict{(E \x A)}{\orig U} \to \Restrict{(E \x A)}{\recomp U} \to C \to \Delta B \x C)
   \) 
 \item \(
   \exists \orig{W}. \exists \recomp{W}. 
   (\Restrict{E}{\orig{W}} \to \FunObj{A}{B} \x C') \x 
   (\Delta \Restrict{E}{\orig W} \to \Restrict{E}{\recomp W} \to C' \to \Delta (\FunObj{A}{B}) \x C')
   \)
\end{itemize} 
Again, we allow ourselves to expose $C$. 
Considering meaning of uses, there is no room to chose $\orig{W}$ and $\recomp{W}$; they must be tails of $\orig{U}$ and $\recomp{U}$, respectively. This consideration suggests
that exposing $C$ is not enough: $\FunObj{A}{B}$ and $\Delta (\FunObj{A}{B})$ must share the information of whether $A$ is used or not.  
If we apply a similar discussion, we would have: 
\begin{align*}
\FunObj{A}{B} &= (\Restrict{A}{\orig{U_A}} \to B \x C) \x (\Restrict{\Delta A}{\orig{U_A}} \to \Restrict{E}{\recomp{W}} \to \Restrict{A}{\recomp{U_A}} \to C \to \Delta B \x C)\\
\Delta (\FunObj{A}{B}) &= \Restrict{A}{\recomp{U_A}} \to C \to \Delta B \x C
\end{align*}
However, this is bad as $\Restrict{E}{\recomp{W}}$ part must be the same type as $\Restrict{E}{\recomp{W}}$ in the outside. 
So, we take a closure approach: 
\begin{align*}
\FunObj{A}{B} &= \Restrict{E}{\orig{W}} \x    
                (\Restrict{(E \x A)}{\orig{U}} \to {B} \x C) \x 
                (\Delta \Restrict{(E \x A)}{\orig U} \to \Restrict{(E \x A)}{\recomp U} \to C \to \Delta B \x C)\\
\Delta (\FunObj{A}{B}) &= \Delta (\Restrict{E}{\orig{W}}) 
\end{align*}
However, even with this approach, we cannot make use of the $\Restrict{(E \x A)}{\recomp U}$ part; recomputation is necessary as no one pass appropriate $\Restrict{E}{\recomp U}$.



% The discussions in Section~\ref{sec:notes-on-function-objects} suggest that it is difficult (at least, far from straightforward) to have a function object (respecting types) 
% in CTS. So, here we expose some internal details to produce function-like objects (we say ``function-like'' as it contains a type parameter cannot determined only by $A$ and $B$). 
% Let us prepare the following types. 
% \begin{align*}
%   \con{Sem} \A E \A A &= \exists C. \exists \orig{U}. \exists \recomp{U}. 
%   (\Restrict{E}{\orig{U}} \to {A} \x C) \x 
%   (\Delta \Restrict{E}{\orig U} \to \Restrict{E}{\recomp U} \to C \to \Delta A \x C)\\
%   \con{Closure} \A E \A C \A A \A B &= E \x \con{Sem} \A (E \x A) \A B\\
%   \Delta(\con{Closure} \A E \A C \A A \A B) &= \Delta E
% \end{align*}
% Here, we assumed that we can lift the restriction operator to semantic level as $\Restrict{E}{U}$. 
% % Now it is easy to have functions of the following signatures. 
% Now it is easy to have a function of the following signature. 
% \[
% \bb 
% \var{plam} &:: \con{Sem} \A \sem{\Gamma, x : A} \A \sem{B} \to \exists E. \exists C. \, \con{Sem} \A \sem{\Gamma} \A (\con{Closure} \A E \A C \A A \A B)\\
% % \var{papp} &:: \forall E. \forall C.\, \con{Sem} \A \sem{\Gamma} \A (\con{Closure} \A E \A C \A A \A B) \to \con{Sem} \A \sem{\Gamma} \A \sem{A} \to \con{Sem} \A \sem{\Gamma} \A \sem{B}
% \ee
% \]
% Accordingly, the $\var{map}$ API is implemented via: 
% \[
% \var{mapSem}'' : \forall E. \forall C. \, \con{Sem} \A \sem{\Gamma} \A (\con{Closure} \A E \A C \A A \A B) \to \sem{ \Gamma, y : \con{Seq} \A A \vdash \con{Seq} \A b }
% \]



\bibliographystyle{plainnat}
\bibliography{main}

\end{document}